import cv2
import mediapipe as mp
import pyautogui
import numpy as np
import time
import os

# Optional: Suppress TensorFlow warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.8,
    min_tracking_confidence=0.8,
    model_complexity=0
)
mp_drawing = mp.solutions.drawing_utils

# Screen dimensions
screen_w, screen_h = pyautogui.size()

# Camera setup
cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)

# Smoothing cursor movement
prev_x, prev_y = 0, 0
smoothing_factor = 0.8

# Timing control
last_click_time = 0
click_delay = 0.5  # seconds
drag_start_time = None
dragging = False

def is_pinch(landmarks):
    index_tip = landmarks[8]
    thumb_tip = landmarks[4]
    distance = np.sqrt((index_tip.x - thumb_tip.x)**2 + (index_tip.y - thumb_tip.y)**2)
    return distance < 0.04

def is_fist(landmarks):
    palm_center = landmarks[9]
    fingertips = [landmarks[8], landmarks[12], landmarks[16], landmarks[20]]
    return all(
        np.sqrt((ft.x - palm_center.x)**2 + (ft.y - palm_center.y)**2) < 0.05
        for ft in fingertips
    )

while cap.isOpened():
    success, frame = cap.read()
    if not success:
        continue

    # Flip frame horizontally
    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Process frame
    results = hands.process(rgb_frame)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            # Draw landmarks
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # Get fingertip coordinates
            fingertip = hand_landmarks.landmark[8]
            x = int(fingertip.x * screen_w)
            y = int(fingertip.y * screen_h)

            # Smooth movement
            smooth_x = prev_x + (x - prev_x) * smoothing_factor
            smooth_y = prev_y + (y - prev_y) * smoothing_factor

            pyautogui.moveTo(smooth_x, smooth_y)
            prev_x, prev_y = smooth_x, smooth_y

            # Gesture detection
            current_time = time.time()

            if is_pinch(hand_landmarks.landmark):
                if drag_start_time is None:
                    drag_start_time = current_time
                elif current_time - drag_start_time > 1.0 and not dragging:
                    pyautogui.mouseDown()  # Start dragging
                    dragging = True
                    cv2.putText(frame, "DRAGGING", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)

                if dragging:
                    cv2.putText(frame, "DRAGGING...", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
                else:
                    if current_time - last_click_time > click_delay:
                        pyautogui.click()  # Left click
                        last_click_time = current_time
                        cv2.putText(frame, "LEFT CLICK", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            else:
                drag_start_time = None
                if dragging:
                    pyautogui.mouseUp()
                    dragging = False

            if is_fist(hand_landmarks.landmark):
                if current_time - last_click_time > click_delay:
                    pyautogui.rightClick()  # Right click
                    last_click_time = current_time
                    cv2.putText(frame, "RIGHT CLICK", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

    # Only HandTrackingView
    cv2.imshow('Hand Tracking View', frame)

    if cv2.waitKey(5) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
